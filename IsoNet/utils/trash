                    if rank == np.random.randint(0, world_size):
                        debug_matrix(pred_y, filename='debug_pred_y.mrc')
                        debug_matrix(mw_rotated_subtomos, filename='debug_mw_rotated_subtomos.mrc')
                        debug_matrix(x2_rot, filename='debug_x2_rot.mrc')
                        debug_matrix(mw, filename='debug_mw.mrc')
                        debug_matrix(rotated_mw, filename='debug_rotated_mw.mrc')
                    loss = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])
                    x [B, C, Z, Y, X]
                    x1, x2, mw = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()
                    std_org = x1.std()
                    mean_org = x1.mean()

                    optimizer.zero_grad(set_to_none=True)
                    # TODO whether need to apply wedge to x1
                    with torch.no_grad():
                        preds = model(x1)
                    #mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    subtomos = apply_F_filter_torch(preds, 1-mw) + apply_F_filter_torch(x1, mw)
                    #for k, d in enumerate(preds):
                        # d [C, Z, Y, X]
                        #outside_mw = torch.real(torch.fft.ifftn((1-mwshift[k])*torch.fft.fftn(preds[k][0])))
                        #inside_mw = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(x1[k][0])))
                        #subtomos[k] = outside_mw + inside_mw

                    #mw_rotated_subtomos = torch.zeros_like(preds)
                    #x2_rot = torch.zeros_like(preds)
                    #rotated_mw = torch.zeros_like(mw)

                    rot = random.choice(rotation_list)
                    rotated_subtomo = rotate_vol(subtomos, rot)
                    mw_rotated_subtomos=apply_F_filter_torch(rotated_subtomo,mw)
                    print(mw_rotated_subtomos.shape)
                    rotated_mw = rotate_vol(mw, rot)
                    x2_rot = rotate_vol(x2, rot)
                    # for k, d in enumerate(preds):
                    #     rot = random.choice(rotation_list)
                    #     rotated_subtomo_k = rotate_vol(subtomos[k][0], rot)
                    #     mw_rotated_subtomos[k][0].copy_(torch.real(torch.fft.ifftn(mwshift[k] * torch.fft.fftn(rotated_subtomo_k))))
                    #     rotated_mw[k].copy_(rotate_vol(mw[k], rot))
                    #     x2_rot[k][0].copy_(rotate_vol(x2[k][0], rot))


                    mw_rotated_subtomos = (mw_rotated_subtomos - mw_rotated_subtomos.mean())/mw_rotated_subtomos.std() \
                                                *std_org + mean_org
                    
                    #pred_y = model(mw_rotated_subtomos)
                    pred_y_new = torch.zeros_like(preds)
                    for k, mw_r in enumerate(rotated_mw):
                        mw_r_shift = torch.fft.fftshift(mw_r, dim=(-1, -2, -3))
                        pred_y_new[k][0].copy_(torch.real(torch.fft.ifftn(mw_r_shift * torch.fft.fftn(pred_y[k][0]))))
                    # if i_batch%100 == 0:
                    #     print(f"x1 {x1.mean()} {x1.std()}")
                    #     print(f"x2 {x2.mean()} {x2.std()}")
                    #     print(f"preds {preds.mean()} {preds.std()}")
                    #     print(f"x2_rot {x2_rot.mean()} {x2_rot.std()}")
                    #     print(f"pred_y {pred_y.mean()} {pred_y.std()}")
                    #     print(f"pred_y_new {pred_y_new.mean()} {pred_y_new.std()}")
                    loss = loss_fn(pred_y_new,x2_rot)
                    # if rank == np.random.randint(0, world_size):
                    #     debug_matrix(pred_y, filename='debug_pred_y.mrc')
                    #     debug_matrix(mw_rotated_subtomos, filename='debug_mw_rotated_subtomos.mrc')
                    #     debug_matrix(x2_rot, filename='debug_x2_rot.mrc')
                    #     debug_matrix(mw, filename='debug_mw.mrc')
                    #     debug_matrix(rotated_mw, filename='debug_rotated_mw.mrc')
                    #loss = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])



                        def refine_spisonet(self, 
                   star_file: str,
                   gpuID: str=None,

                   #ncpus: int=16, 
                   output_dir: str="isonet_maps",
                   pretrained_model: str=None,
                   cube_size: int=96,

                   epochs: int=50,
                   batch_size: int=None, 
                   acc_batches: int=2,
                   learning_rate: float=3e-4,
                   alpha: float=1
                   ):
        from IsoNet.utils.utils import mkfolder
        mkfolder(output_dir)

        ngpus, gpuID, gpuID_list=parse_gpu(gpuID)

        if batch_size is None:
            if ngpus == 1:
                batch_size = 4
            else:
                batch_size = 2 * len(gpuID_list)
        steps_per_epoch = 200

        from IsoNet.models.network import Net
        network = Net(method="spisonet-single", arch='unet-default')
        if pretrained_model != None and pretrained_model != "None":
            network.load(pretrained_model)

        mixed_precision = False

        training_params = {
            "data_path":star_file,
            "output_dir":output_dir,
            "batch_size":batch_size,
            "acc_batches": acc_batches,
            "epochs": epochs,
            "steps_per_epoch":steps_per_epoch,
            "learning_rate":learning_rate,
            "mixed_precision":mixed_precision,
            "cube_size": cube_size,
        }

        network.train(training_params) #train based on init model and save new one as model_iter{num_iter}.h5



                elif training_params['method'] == "spisonet":

                    x1, x2 = batch[0], batch[1]
                    x1 = x1.cuda()
                    x2 = x2.cuda()
                    optimizer.zero_grad(set_to_none=True)

                    
                    preds = model(x1)  
                    mw = batch[2].cuda()
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift[j]*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss_consistency_1 = loss_fn(data,x1)

                    if training_params['beta'] > 0:
                        loss_consistency_2 = loss_fn(data,x2)
                        pred_2 = model(x2)
                        data_rot_2 = torch.zeros_like(pred_2)   
                    else:
                        loss_consistency_2 = 0
                        training_params['beta'] = 0

                    data_rot = torch.zeros_like(preds)
                    data_e = torch.zeros_like(preds)
                    for k,d in enumerate(preds):
                        rot = random.choice(rotation_list_24)
                        tmp = torch.rot90(d[0],rot[0][1],rot[0][0])
                        data_rot[k][0] = torch.rot90(tmp,rot[1][1],rot[1][0])
                        if training_params['beta'] > 0:
                            tmp_2 = torch.rot90(pred_2[k][0],rot[0][1],rot[0][0])
                            data_rot_2[k][0] = torch.rot90(tmp_2,rot[1][1],rot[1][0])
                        data_e[k][0] = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(data_rot[k][0])))#+noise[i][0]#.astype(np.float32)
                    pred_y = model(data_e)
                    loss_equivariance_1 = loss_fn(pred_y, data_rot)

                    if training_params['beta'] > 0:
                        loss_equivariance_2 = loss_fn(pred_y, data_rot_2)
                    else:
                        loss_equivariance_2 = 0
                    loss = training_params['alpha'] * loss_equivariance_1 + loss_consistency_1 + \
                           training_params['beta'] * ( training_params['alpha'] * loss_equivariance_2 + loss_consistency_2)
                elif training_params['method'] == "spisonet-single":
                    x1, x2 = batch[0], batch[1]
                    x1 = x1.cuda()
                    x2 = x2.cuda()
                    optimizer.zero_grad(set_to_none=True)

                    
                    preds = model(x1)  
                    mw = x2
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss_consistency_1 = loss_fn(data,x1)
                    data_rot = torch.zeros_like(preds)
                    data_e = torch.zeros_like(preds)
                    for k,d in enumerate(preds):
                        rot = random.choice(rotation_list_24)
                        tmp = torch.rot90(d[0],rot[0][1],rot[0][0])
                        data_rot[k][0] = torch.rot90(tmp,rot[1][1],rot[1][0])
                        data_e[k][0] = torch.real(torch.fft.ifftn(mwshift*torch.fft.fftn(data_rot[k][0])))#+noise[i][0]#.astype(np.float32)
                    pred_y = model(data_e)
                    loss_equivariance_1 = loss_fn(pred_y, data_rot)
                    loss = training_params['alpha'] * loss_equivariance_1 + loss_consistency_1

                elif training_params['method'] == "spisonet-ddw-legacy":
                    x1, x2 = batch[0], batch[1]
                    x1 = x1.cuda()
                    x2 = x2.cuda()
                    optimizer.zero_grad(set_to_none=True)

                    #with torch.no_grad():
                    preds = model(x1)  
                    mw = batch[2].cuda()
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    if rank == np.random.randint(0, world_size):
                        debug_matrix(mwshift, filename='debugmwshift.mrc')

                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift[j]*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss = loss_fn(data,x2)

                    if training_params['alpha'] > 0:
                        data_combine_rot_mw = torch.empty_like(preds)
                        x2_rot = torch.empty_like(preds)
                        rotated_mw = torch.empty_like(mw)
                        new_data = preds - data + x1
                        for k,d in enumerate(preds):
                            rot = random.choice(rotation_list_24)


                            #outside_mw = torch.real(torch.fft.ifftn((1-mwshift[k])*torch.fft.fftn(preds[k][0])))
                            #inside_mw = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(x1[k][0])))
                            tmp = new_data[k][0]#outside_mw + inside_mw                   
                            tmp = torch.rot90(tmp,rot[0][1],rot[0][0])
                            tmp = torch.rot90(tmp,rot[1][1],rot[1][0])
                            data_combine_rot_mw[k][0] = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(tmp)))

                            tmp_mw = torch.rot90(mw[k],rot[0][1],rot[0][0])
                            rotated_mw[k] = torch.rot90(tmp_mw,rot[1][1],rot[1][0])

                            tmp_x2_rot = torch.rot90(x2[k][0],rot[0][1],rot[0][0])
                            x2_rot[k][0] = torch.rot90(tmp_x2_rot,rot[1][1],rot[1][0])


                        pred_y = model(data_combine_rot_mw)
                        # if rank == np.random.randint(0, world_size):
                        #     debug_matrix(x1, filename='debug1.mrc')
                        #     debug_matrix(x2, filename='debug2.mrc')
                        #     debug_matrix(preds, filename='debug3.mrc')   
                        #     debug_matrix(x2_rot, filename='debug4.mrc')
                        #     debug_matrix(data_combine_rot_mw, filename='debug5.mrc')
                        #     debug_matrix(pred_y, filename='debug6.mrc')
                        #     debug_matrix(rotated_mw, filename='debug7.mrc')
                        #loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, torch.ones_like(mw), mw_weight=2.0)
                        loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])


                        loss += loss_ddw * training_params['alpha']
                elif training_params['method'] == "spisonet-ddw-test":
                    x1, x2 = batch[0].cuda(), batch[1].cuda()
                    optimizer.zero_grad(set_to_none=True)

                    #with torch.no_grad():
                    preds = model(x1)  
                    mw = batch[2].cuda()
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))

                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift[j]*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss = loss_fn(data,x2)

                    if training_params['alpha'] > 0:
                        data_combine_rot_mw = torch.empty_like(preds)
                        x2_rot = torch.empty_like(preds)
                        rotated_mw = torch.empty_like(mw)
                        new_data = preds - data + x1
                        for k,d in enumerate(preds):
                            rot = random.choice(rotation_list_24)

                            #outside_mw = torch.real(torch.fft.ifftn((1-mwshift[k])*torch.fft.fftn(preds[k][0])))
                            #inside_mw = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(x1[k][0])))
                            
                            rotated_data = rotate_vol(new_data[k][0], rot)          
                            data_combine_rot_mw[k][0] = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(rotated_data)))

                            rotated_mw[k] = rotate_vol(mw[k], rot)
                            x2_rot[k][0] = rotate_vol(x2[k][0], rot)


                        pred_y = model(data_combine_rot_mw)

                        loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])


                        loss += loss_ddw * training_params['alpha']

                    

                elif training_params['method'] == "spisonet-ddw-test_v3":
                    x1, x2 = batch[0].cuda(), batch[1].cuda()
                    mw = batch[2].cuda()
                    optimizer.zero_grad(set_to_none=True)

                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))

                    # Probability calculation for choosing the training path
                    prob = 1 / (1 + training_params['alpha'])

                    # Determine `first_half` on rank 0 and synchronize across all GPUs
                    if torch.distributed.get_rank() == 0:
                        random_number = np.random.rand()
                        first_half = torch.tensor(random_number < prob, dtype=torch.bool, device='cuda')  # On rank 0, initialize first_half as a tensor
                    else:
                        first_half = torch.tensor(False, dtype=torch.bool, device='cuda')  # On other ranks, initialize first_half as False

                    torch.distributed.barrier()

                    # Broadcast `first_half` decision from rank 0 to all GPUs
                    #first_half = torch.tensor(first_half, dtype=torch.bool, device='cuda')
                    torch.distributed.broadcast(first_half, src=0)
                    first_half = first_half.item() 

                    # Forward pass
                    if first_half:
                        preds = model(x1)
                        data = torch.zeros_like(preds)
                        for j, d in enumerate(preds):
                            data[j][0].copy_(torch.real(torch.fft.ifftn(mwshift[j] * torch.fft.fftn(d[0]))))
                        loss = loss_fn(data, x2)

                    else:
                        with torch.no_grad():
                            preds = model(x1)
                        data = torch.zeros_like(preds)
                        for j, d in enumerate(preds):
                            data[j][0].copy_(torch.real(torch.fft.ifftn(mwshift[j] * torch.fft.fftn(d[0]))))

                        # Reuse tensors for augmentation step
                        data_combine_rot_mw = torch.zeros_like(preds)
                        x2_rot = torch.zeros_like(preds)
                        rotated_mw = torch.zeros_like(mw)
                        new_data = preds - data + x1

                        for k, d in enumerate(preds):
                            rot = random.choice(rotation_list_24)

                            # Rotate volumes and update in-place
                            rotated_data = rotate_vol(new_data[k][0], rot)
                            data_combine_rot_mw[k][0].copy_(torch.real(torch.fft.ifftn(mwshift[k] * torch.fft.fftn(rotated_data))))

                            rotated_mw[k].copy_(rotate_vol(mw[k], rot))
                            x2_rot[k][0].copy_(rotate_vol(x2[k][0], rot))

                        # Compute augmented loss
                        pred_y = model(data_combine_rot_mw)
                        loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])
                        loss = loss_ddw
def rotate_cubes(data):
    rotated_data = np.zeros((len(rotation_list), *data.shape))
    old_rotation = True
    if old_rotation:
        for i,r in enumerate(rotation_list):
            data_copy = np.rot90(data, k=r[0][1], axes=r[0][0])
            data_copy = np.rot90(data_copy, k=r[1][1], axes=r[1][0])
            rotated_data[i] = data_copy
    else:
        from scipy.ndimage import affine_transform
        from scipy.stats import special_ortho_group 
        for i in range(len(rotation_list)):
            rot = special_ortho_group.rvs(3)
            center = (np.array(data.shape) -1 )/2.
            offset = center-np.dot(rot,center)
            rotated_data[i] = affine_transform(data,rot,offset=offset)
    return rotated_data


def ddp_train(rank, world_size, port_number, model, training_params):
    #data_path, batch_size, acc_batches, epochs, steps_per_epoch, learning_rate, mixed_precision, model_path
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = port_number
    #os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    

    if world_size > 1:
        dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
        torch.cuda.set_device(rank)
        model = nn.SyncBatchNorm.convert_sync_batchnorm(model)
        model = model.to(rank)
        model = DDP(model, device_ids=[rank])
    else:
        model = model.to(rank)

    batch_size_gpu = training_params['batch_size'] // (training_params['acc_batches'] * world_size)

    #### preparing data
    # from chatGPT: The DistributedSampler shuffles the indices of the entire dataset, not just the portion assigned to a specific GPU. 
    if training_params['method'] == 'regular':
        from IsoNet.models.data_sequence import Train_sets_regular
        train_dataset = Train_sets_regular(training_params['star_file'])

    elif training_params['method'] in ['n2n', 'isonet2', 'isonet2-n2n']:
        from IsoNet.models.data_sequence import Train_sets_n2n
        if rank == 0:
            print("calculate subtomograms position")
        train_dataset = Train_sets_n2n(training_params['star_file'],method=training_params['method'], 
                                       cube_size=training_params['cube_size'], input_column=training_params['input_column'])
        

    if world_size > 1:
        train_sampler = DistributedSampler(train_dataset, shuffle=True, drop_last=True)
    else:
        train_sampler = None  # No sampler for single GPU

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=batch_size_gpu, persistent_workers=True,
        num_workers=training_params["ncpus"], pin_memory=True, sampler=train_sampler)
    
    if training_params['compile_model'] == True:
        if torch.__version__ >= "2.0.0":
            GPU_capability = torch.cuda.get_device_capability()
            if GPU_capability[0] >= 7:
                torch.set_float32_matmul_precision('high')
                model = torch.compile(model)
    optimizer = torch.optim.AdamW(model.parameters(), lr=training_params['learning_rate'])
    loss_fn = nn.MSELoss()
    #torch.backends.cuda.matmul.allow_tf32 = True
    #torch.backends.cudnn.allow_tf32 = True

    if training_params['mixed_precision']:
        scaler = torch.cuda.amp.GradScaler()
    
    average_loss_list = []

    steps_per_epoch_train = training_params['steps_per_epoch']
    total_steps = min(len(train_loader)//training_params['acc_batches'], training_params['steps_per_epoch'])

    for epoch in range(training_params['epochs']):
        if train_sampler:
            train_sampler.set_epoch(epoch)
        model.train()
        with tqdm(total=total_steps, unit="batch", disable=(rank!=0)) as progress_bar:
            
            # have to convert to tensor because reduce needed it
            average_loss = torch.tensor(0, dtype=torch.float).to(rank)
            for i_batch, batch in enumerate(train_loader):  
                
                if training_params['method'] in ["n2n", "regular"]:
                    x1, x2 = batch[0], batch[1]
                    x1 = x1.cuda()
                    x2 = x2.cuda()
                    optimizer.zero_grad(set_to_none=True)          

                    if training_params["mixed_precision"]:
                        with torch.cuda.amp.autocast():  # Mixed precision forward pass
                            preds = model(x1)
                            loss = loss_fn(x2, preds)    
                    else:                            
                        preds = model(x1)  
                        loss = loss_fn(x2,preds)

                elif training_params['method'] in ["isonet2",'isonet2-n2n']:
                    # x [B, C, Z, Y, X]
                    x1, x2, mw = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()
                    if training_params['random_rotation'] == True:
                        rotate = rotate_vol_around_axis_torch
                        rot = sample_rot_axis_and_angle()
                    else:
                        rotate = rotate_vol
                        rot = random.choice(rotation_list)

                    optimizer.zero_grad(set_to_none=True)

                    std_org = x1.std()
                    mean_org = x1.mean()

                    # TODO whether need to apply wedge to x1
                    with torch.no_grad():
                        if training_params["mixed_precision"]:
                            with torch.cuda.amp.autocast():  # Mixed precision forward pass
                                preds = model(x1)
                        else:
                            preds = model(x1)
                    preds = preds.to(torch.float32)
                    if training_params['apply_mw_x1']:
                        subtomos = apply_F_filter_torch(preds, 1-mw) + apply_F_filter_torch(x1, mw)
                    else:
                        subtomos = apply_F_filter_torch(preds, 1-mw) + x1

                    rotated_subtomo = rotate(subtomos, rot)
                    mw_rotated_subtomos=apply_F_filter_torch(rotated_subtomo,mw)
                    rotated_mw = rotate(mw, rot)
                    x2_rot = rotate(x2, rot)
                    mw_rotated_subtomos = (mw_rotated_subtomos - mw_rotated_subtomos.mean())/mw_rotated_subtomos.std() \
                                                *std_org + mean_org
                    
                    if training_params["mixed_precision"]:
                        with torch.cuda.amp.autocast():  # Mixed precision forward pass
                            pred_y = model(mw_rotated_subtomos)
                            if training_params['gamma'] > 0:
                                loss = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])
                            else:
                                pred_y_new = apply_F_filter_torch(pred_y,rotated_mw)
                                loss = loss_fn(pred_y_new,x2_rot)
                    else:
                        pred_y = model(mw_rotated_subtomos)
                        if training_params['gamma'] > 0:
                            loss = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])
                        else:
                            pred_y_new = apply_F_filter_torch(pred_y,rotated_mw)
                            loss = loss_fn(pred_y_new,x2_rot)
                    # if i_batch%100 == 0:
                    #     print(f"x1 {x1.mean()} {x1.std()}")
                    #     print(f"x2 {x2.mean()} {x2.std()}")
                    #     print(f"preds {preds.mean()} {preds.std()}")
                    #     print(f"x2_rot {x2_rot.mean()} {x2_rot.std()}")
                    #     print(f"pred_y {pred_y.mean()} {pred_y.std()}")
                    #     print(f"pred_y_new {pred_y_new.mean()} {pred_y_new.std()}")
                    # if rank == np.random.randint(0, world_size):
                    #     debug_matrix(x1, filename='debug_x1.mrc')
                    #     debug_matrix(subtomos, filename='debug_subtomos.mrc')
                    #     debug_matrix(pred_y, filename='debug_pred_y.mrc')
                    #     debug_matrix(pred_y_new, filename='debug_pred_y_new.mrc')
                    #     debug_matrix(rotated_subtomo, filename='debug_rotated_subtomo.mrc')
                    #     debug_matrix(mw_rotated_subtomos, filename='debug_mw_rotated_subtomos.mrc')
                    #     debug_matrix(x2_rot, filename='debug_x2_rot.mrc')
                    #     debug_matrix(mw, filename='debug_mw.mrc')
                    #     debug_matrix(rotated_mw, filename='debug_rotated_mw.mrc')

                
                loss = loss / training_params['acc_batches']
                if training_params['mixed_precision']:
                    scaler.scale(loss).backward()  # Scaled backward pass
                else:
                    loss.backward()  # Normal backward pass
                #loss.backward()
                loss_item = loss.item()
                              
                if ( (i_batch+1)%training_params['acc_batches'] == 0 ) or (i_batch+1) == min(len(train_loader), steps_per_epoch_train * training_params['acc_batches']):
                    if training_params['mixed_precision']:
                        # Unscale the gradients and apply the optimizer step
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()

                if rank == 0 and ( (i_batch+1)%training_params['acc_batches'] == 0 ):
                   progress_bar.set_postfix({"Loss": loss_item})#, "t1": time2-time1, "t2": time3-time2, "t3": time4-time3})
                   progress_bar.update()
                average_loss += loss_item
                
                if i_batch + 1 >= steps_per_epoch_train*training_params['acc_batches']:
                    break

        # Normalize loss across GPUs
        if world_size > 1:
            dist.barrier()
            dist.reduce(average_loss, dst=0)
            average_loss /= (world_size * (i_batch + 1))
        else:
            average_loss /= (i_batch + 1)