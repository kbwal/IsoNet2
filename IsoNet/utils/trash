                    if rank == np.random.randint(0, world_size):
                        debug_matrix(pred_y, filename='debug_pred_y.mrc')
                        debug_matrix(mw_rotated_subtomos, filename='debug_mw_rotated_subtomos.mrc')
                        debug_matrix(x2_rot, filename='debug_x2_rot.mrc')
                        debug_matrix(mw, filename='debug_mw.mrc')
                        debug_matrix(rotated_mw, filename='debug_rotated_mw.mrc')
                    loss = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])
                    x [B, C, Z, Y, X]
                    x1, x2, mw = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()
                    std_org = x1.std()
                    mean_org = x1.mean()

                    optimizer.zero_grad(set_to_none=True)
                    # TODO whether need to apply wedge to x1
                    with torch.no_grad():
                        preds = model(x1)
                    #mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    subtomos = apply_F_filter_torch(preds, 1-mw) + apply_F_filter_torch(x1, mw)
                    #for k, d in enumerate(preds):
                        # d [C, Z, Y, X]
                        #outside_mw = torch.real(torch.fft.ifftn((1-mwshift[k])*torch.fft.fftn(preds[k][0])))
                        #inside_mw = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(x1[k][0])))
                        #subtomos[k] = outside_mw + inside_mw

                    #mw_rotated_subtomos = torch.zeros_like(preds)
                    #x2_rot = torch.zeros_like(preds)
                    #rotated_mw = torch.zeros_like(mw)

                    rot = random.choice(rotation_list)
                    rotated_subtomo = rotate_vol(subtomos, rot)
                    mw_rotated_subtomos=apply_F_filter_torch(rotated_subtomo,mw)
                    print(mw_rotated_subtomos.shape)
                    rotated_mw = rotate_vol(mw, rot)
                    x2_rot = rotate_vol(x2, rot)
                    # for k, d in enumerate(preds):
                    #     rot = random.choice(rotation_list)
                    #     rotated_subtomo_k = rotate_vol(subtomos[k][0], rot)
                    #     mw_rotated_subtomos[k][0].copy_(torch.real(torch.fft.ifftn(mwshift[k] * torch.fft.fftn(rotated_subtomo_k))))
                    #     rotated_mw[k].copy_(rotate_vol(mw[k], rot))
                    #     x2_rot[k][0].copy_(rotate_vol(x2[k][0], rot))


                    mw_rotated_subtomos = (mw_rotated_subtomos - mw_rotated_subtomos.mean())/mw_rotated_subtomos.std() \
                                                *std_org + mean_org
                    
                    #pred_y = model(mw_rotated_subtomos)
                    pred_y_new = torch.zeros_like(preds)
                    for k, mw_r in enumerate(rotated_mw):
                        mw_r_shift = torch.fft.fftshift(mw_r, dim=(-1, -2, -3))
                        pred_y_new[k][0].copy_(torch.real(torch.fft.ifftn(mw_r_shift * torch.fft.fftn(pred_y[k][0]))))
                    # if i_batch%100 == 0:
                    #     print(f"x1 {x1.mean()} {x1.std()}")
                    #     print(f"x2 {x2.mean()} {x2.std()}")
                    #     print(f"preds {preds.mean()} {preds.std()}")
                    #     print(f"x2_rot {x2_rot.mean()} {x2_rot.std()}")
                    #     print(f"pred_y {pred_y.mean()} {pred_y.std()}")
                    #     print(f"pred_y_new {pred_y_new.mean()} {pred_y_new.std()}")
                    loss = loss_fn(pred_y_new,x2_rot)
                    # if rank == np.random.randint(0, world_size):
                    #     debug_matrix(pred_y, filename='debug_pred_y.mrc')
                    #     debug_matrix(mw_rotated_subtomos, filename='debug_mw_rotated_subtomos.mrc')
                    #     debug_matrix(x2_rot, filename='debug_x2_rot.mrc')
                    #     debug_matrix(mw, filename='debug_mw.mrc')
                    #     debug_matrix(rotated_mw, filename='debug_rotated_mw.mrc')
                    #loss = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])



                        def refine_spisonet(self, 
                   star_file: str,
                   gpuID: str=None,

                   #ncpus: int=16, 
                   output_dir: str="isonet_maps",
                   pretrained_model: str=None,
                   cube_size: int=96,

                   epochs: int=50,
                   batch_size: int=None, 
                   acc_batches: int=2,
                   learning_rate: float=3e-4,
                   alpha: float=1
                   ):
        from IsoNet.utils.utils import mkfolder
        mkfolder(output_dir)

        ngpus, gpuID, gpuID_list=parse_gpu(gpuID)

        if batch_size is None:
            if ngpus == 1:
                batch_size = 4
            else:
                batch_size = 2 * len(gpuID_list)
        steps_per_epoch = 200

        from IsoNet.models.network import Net
        network = Net(method="spisonet-single", arch='unet-default')
        if pretrained_model != None and pretrained_model != "None":
            network.load(pretrained_model)

        mixed_precision = False

        training_params = {
            "data_path":star_file,
            "output_dir":output_dir,
            "batch_size":batch_size,
            "acc_batches": acc_batches,
            "epochs": epochs,
            "steps_per_epoch":steps_per_epoch,
            "learning_rate":learning_rate,
            "mixed_precision":mixed_precision,
            "cube_size": cube_size,
        }

        network.train(training_params) #train based on init model and save new one as model_iter{num_iter}.h5



                elif training_params['method'] == "spisonet":

                    x1, x2 = batch[0], batch[1]
                    x1 = x1.cuda()
                    x2 = x2.cuda()
                    optimizer.zero_grad(set_to_none=True)

                    
                    preds = model(x1)  
                    mw = batch[2].cuda()
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift[j]*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss_consistency_1 = loss_fn(data,x1)

                    if training_params['beta'] > 0:
                        loss_consistency_2 = loss_fn(data,x2)
                        pred_2 = model(x2)
                        data_rot_2 = torch.zeros_like(pred_2)   
                    else:
                        loss_consistency_2 = 0
                        training_params['beta'] = 0

                    data_rot = torch.zeros_like(preds)
                    data_e = torch.zeros_like(preds)
                    for k,d in enumerate(preds):
                        rot = random.choice(rotation_list_24)
                        tmp = torch.rot90(d[0],rot[0][1],rot[0][0])
                        data_rot[k][0] = torch.rot90(tmp,rot[1][1],rot[1][0])
                        if training_params['beta'] > 0:
                            tmp_2 = torch.rot90(pred_2[k][0],rot[0][1],rot[0][0])
                            data_rot_2[k][0] = torch.rot90(tmp_2,rot[1][1],rot[1][0])
                        data_e[k][0] = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(data_rot[k][0])))#+noise[i][0]#.astype(np.float32)
                    pred_y = model(data_e)
                    loss_equivariance_1 = loss_fn(pred_y, data_rot)

                    if training_params['beta'] > 0:
                        loss_equivariance_2 = loss_fn(pred_y, data_rot_2)
                    else:
                        loss_equivariance_2 = 0
                    loss = training_params['alpha'] * loss_equivariance_1 + loss_consistency_1 + \
                           training_params['beta'] * ( training_params['alpha'] * loss_equivariance_2 + loss_consistency_2)
                elif training_params['method'] == "spisonet-single":
                    x1, x2 = batch[0], batch[1]
                    x1 = x1.cuda()
                    x2 = x2.cuda()
                    optimizer.zero_grad(set_to_none=True)

                    
                    preds = model(x1)  
                    mw = x2
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss_consistency_1 = loss_fn(data,x1)
                    data_rot = torch.zeros_like(preds)
                    data_e = torch.zeros_like(preds)
                    for k,d in enumerate(preds):
                        rot = random.choice(rotation_list_24)
                        tmp = torch.rot90(d[0],rot[0][1],rot[0][0])
                        data_rot[k][0] = torch.rot90(tmp,rot[1][1],rot[1][0])
                        data_e[k][0] = torch.real(torch.fft.ifftn(mwshift*torch.fft.fftn(data_rot[k][0])))#+noise[i][0]#.astype(np.float32)
                    pred_y = model(data_e)
                    loss_equivariance_1 = loss_fn(pred_y, data_rot)
                    loss = training_params['alpha'] * loss_equivariance_1 + loss_consistency_1

                elif training_params['method'] == "spisonet-ddw-legacy":
                    x1, x2 = batch[0], batch[1]
                    x1 = x1.cuda()
                    x2 = x2.cuda()
                    optimizer.zero_grad(set_to_none=True)

                    #with torch.no_grad():
                    preds = model(x1)  
                    mw = batch[2].cuda()
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))
                    if rank == np.random.randint(0, world_size):
                        debug_matrix(mwshift, filename='debugmwshift.mrc')

                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift[j]*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss = loss_fn(data,x2)

                    if training_params['alpha'] > 0:
                        data_combine_rot_mw = torch.empty_like(preds)
                        x2_rot = torch.empty_like(preds)
                        rotated_mw = torch.empty_like(mw)
                        new_data = preds - data + x1
                        for k,d in enumerate(preds):
                            rot = random.choice(rotation_list_24)


                            #outside_mw = torch.real(torch.fft.ifftn((1-mwshift[k])*torch.fft.fftn(preds[k][0])))
                            #inside_mw = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(x1[k][0])))
                            tmp = new_data[k][0]#outside_mw + inside_mw                   
                            tmp = torch.rot90(tmp,rot[0][1],rot[0][0])
                            tmp = torch.rot90(tmp,rot[1][1],rot[1][0])
                            data_combine_rot_mw[k][0] = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(tmp)))

                            tmp_mw = torch.rot90(mw[k],rot[0][1],rot[0][0])
                            rotated_mw[k] = torch.rot90(tmp_mw,rot[1][1],rot[1][0])

                            tmp_x2_rot = torch.rot90(x2[k][0],rot[0][1],rot[0][0])
                            x2_rot[k][0] = torch.rot90(tmp_x2_rot,rot[1][1],rot[1][0])


                        pred_y = model(data_combine_rot_mw)
                        # if rank == np.random.randint(0, world_size):
                        #     debug_matrix(x1, filename='debug1.mrc')
                        #     debug_matrix(x2, filename='debug2.mrc')
                        #     debug_matrix(preds, filename='debug3.mrc')   
                        #     debug_matrix(x2_rot, filename='debug4.mrc')
                        #     debug_matrix(data_combine_rot_mw, filename='debug5.mrc')
                        #     debug_matrix(pred_y, filename='debug6.mrc')
                        #     debug_matrix(rotated_mw, filename='debug7.mrc')
                        #loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, torch.ones_like(mw), mw_weight=2.0)
                        loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])


                        loss += loss_ddw * training_params['alpha']
                elif training_params['method'] == "spisonet-ddw-test":
                    x1, x2 = batch[0].cuda(), batch[1].cuda()
                    optimizer.zero_grad(set_to_none=True)

                    #with torch.no_grad():
                    preds = model(x1)  
                    mw = batch[2].cuda()
                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))

                    data = torch.zeros_like(preds)
                    for j,d in enumerate(preds):
                        data[j][0] = torch.real(torch.fft.ifftn(mwshift[j]*torch.fft.fftn(d[0])))#.astype(np.float32)
                    loss = loss_fn(data,x2)

                    if training_params['alpha'] > 0:
                        data_combine_rot_mw = torch.empty_like(preds)
                        x2_rot = torch.empty_like(preds)
                        rotated_mw = torch.empty_like(mw)
                        new_data = preds - data + x1
                        for k,d in enumerate(preds):
                            rot = random.choice(rotation_list_24)

                            #outside_mw = torch.real(torch.fft.ifftn((1-mwshift[k])*torch.fft.fftn(preds[k][0])))
                            #inside_mw = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(x1[k][0])))
                            
                            rotated_data = rotate_vol(new_data[k][0], rot)          
                            data_combine_rot_mw[k][0] = torch.real(torch.fft.ifftn(mwshift[k]*torch.fft.fftn(rotated_data)))

                            rotated_mw[k] = rotate_vol(mw[k], rot)
                            x2_rot[k][0] = rotate_vol(x2[k][0], rot)


                        pred_y = model(data_combine_rot_mw)

                        loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])


                        loss += loss_ddw * training_params['alpha']

                    

                elif training_params['method'] == "spisonet-ddw-test_v3":
                    x1, x2 = batch[0].cuda(), batch[1].cuda()
                    mw = batch[2].cuda()
                    optimizer.zero_grad(set_to_none=True)

                    mwshift = torch.fft.fftshift(mw, dim=(-1, -2, -3))

                    # Probability calculation for choosing the training path
                    prob = 1 / (1 + training_params['alpha'])

                    # Determine `first_half` on rank 0 and synchronize across all GPUs
                    if torch.distributed.get_rank() == 0:
                        random_number = np.random.rand()
                        first_half = torch.tensor(random_number < prob, dtype=torch.bool, device='cuda')  # On rank 0, initialize first_half as a tensor
                    else:
                        first_half = torch.tensor(False, dtype=torch.bool, device='cuda')  # On other ranks, initialize first_half as False

                    torch.distributed.barrier()

                    # Broadcast `first_half` decision from rank 0 to all GPUs
                    #first_half = torch.tensor(first_half, dtype=torch.bool, device='cuda')
                    torch.distributed.broadcast(first_half, src=0)
                    first_half = first_half.item() 

                    # Forward pass
                    if first_half:
                        preds = model(x1)
                        data = torch.zeros_like(preds)
                        for j, d in enumerate(preds):
                            data[j][0].copy_(torch.real(torch.fft.ifftn(mwshift[j] * torch.fft.fftn(d[0]))))
                        loss = loss_fn(data, x2)

                    else:
                        with torch.no_grad():
                            preds = model(x1)
                        data = torch.zeros_like(preds)
                        for j, d in enumerate(preds):
                            data[j][0].copy_(torch.real(torch.fft.ifftn(mwshift[j] * torch.fft.fftn(d[0]))))

                        # Reuse tensors for augmentation step
                        data_combine_rot_mw = torch.zeros_like(preds)
                        x2_rot = torch.zeros_like(preds)
                        rotated_mw = torch.zeros_like(mw)
                        new_data = preds - data + x1

                        for k, d in enumerate(preds):
                            rot = random.choice(rotation_list_24)

                            # Rotate volumes and update in-place
                            rotated_data = rotate_vol(new_data[k][0], rot)
                            data_combine_rot_mw[k][0].copy_(torch.real(torch.fft.ifftn(mwshift[k] * torch.fft.fftn(rotated_data))))

                            rotated_mw[k].copy_(rotate_vol(mw[k], rot))
                            x2_rot[k][0].copy_(rotate_vol(x2[k][0], rot))

                        # Compute augmented loss
                        pred_y = model(data_combine_rot_mw)
                        loss_ddw = masked_loss(pred_y, x2_rot, rotated_mw, mw, mw_weight=training_params['gamma'])
                        loss = loss_ddw
